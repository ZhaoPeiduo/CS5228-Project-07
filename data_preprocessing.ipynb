{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS5228 Mini Project Team 07 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "This notebook aims to preprocess the raw dataset to:\n",
    "- Handle missing values and perform data cleaning\n",
    "- Encode categorical features\n",
    "- Normalize numerical features\n",
    "\n",
    "The output of this notebook create a pickle file `cleaned_datasets.pkl` containing variants of train and test datasets. Structure:\n",
    "```python\n",
    "{\n",
    "    \"standard_scale\": \n",
    "    {\n",
    "        \"train\": train_df_with_standard_scaling,\n",
    "        \"test\": test_df_with_standard_scaling\n",
    "    },\n",
    "\n",
    "    \"min_max_scale\": \n",
    "    {\n",
    "        \"train\": train_df_with_min_max_scaling,\n",
    "        \"test\": test_df_with_min_max_scaling\n",
    "    } \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./dataset/churn-bigml-80.csv')\n",
    "test_df = pd.read_csv('./dataset/churn-bigml-20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2666 entries, 0 to 2665\n",
      "Data columns (total 20 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   State                   2666 non-null   object \n",
      " 1   Account length          2666 non-null   int64  \n",
      " 2   Area code               2666 non-null   int64  \n",
      " 3   International plan      2666 non-null   object \n",
      " 4   Voice mail plan         2666 non-null   object \n",
      " 5   Number vmail messages   2666 non-null   int64  \n",
      " 6   Total day minutes       2666 non-null   float64\n",
      " 7   Total day calls         2666 non-null   int64  \n",
      " 8   Total day charge        2666 non-null   float64\n",
      " 9   Total eve minutes       2666 non-null   float64\n",
      " 10  Total eve calls         2666 non-null   int64  \n",
      " 11  Total eve charge        2666 non-null   float64\n",
      " 12  Total night minutes     2666 non-null   float64\n",
      " 13  Total night calls       2666 non-null   int64  \n",
      " 14  Total night charge      2666 non-null   float64\n",
      " 15  Total intl minutes      2666 non-null   float64\n",
      " 16  Total intl calls        2666 non-null   int64  \n",
      " 17  Total intl charge       2666 non-null   float64\n",
      " 18  Customer service calls  2666 non-null   int64  \n",
      " 19  Churn                   2666 non-null   bool   \n",
      "dtypes: bool(1), float64(8), int64(8), object(3)\n",
      "memory usage: 398.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 667 entries, 0 to 666\n",
      "Data columns (total 20 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   State                   667 non-null    object \n",
      " 1   Account length          667 non-null    int64  \n",
      " 2   Area code               667 non-null    int64  \n",
      " 3   International plan      667 non-null    object \n",
      " 4   Voice mail plan         667 non-null    object \n",
      " 5   Number vmail messages   667 non-null    int64  \n",
      " 6   Total day minutes       667 non-null    float64\n",
      " 7   Total day calls         667 non-null    int64  \n",
      " 8   Total day charge        667 non-null    float64\n",
      " 9   Total eve minutes       667 non-null    float64\n",
      " 10  Total eve calls         667 non-null    int64  \n",
      " 11  Total eve charge        667 non-null    float64\n",
      " 12  Total night minutes     667 non-null    float64\n",
      " 13  Total night calls       667 non-null    int64  \n",
      " 14  Total night charge      667 non-null    float64\n",
      " 15  Total intl minutes      667 non-null    float64\n",
      " 16  Total intl calls        667 non-null    int64  \n",
      " 17  Total intl charge       667 non-null    float64\n",
      " 18  Customer service calls  667 non-null    int64  \n",
      " 19  Churn                   667 non-null    bool   \n",
      "dtypes: bool(1), float64(8), int64(8), object(3)\n",
      "memory usage: 99.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the overview above, there is no missing entry in both train and test datasets. Now we split numerical and categorical features for further EDA and data preprocessing.\n",
    "\n",
    "Numerical columns: 'Account length ', 'Number vmail messages', 'Total day minutes', 'Total day calls', 'Total day charge', 'Total eve minutes', 'Total eve calls', 'Total eve charge', 'Total night minutes', 'Total night calls', 'Total night charge', 'Total intl minutes', 'Total intl calls', 'Total intl charge', 'Customer service calls'\n",
    "\n",
    "Categorical columns: 'State', 'Area code', 'International plan', 'Voice mail plan', 'Churn'(which is the target column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['State', 'Account length', 'Area code', 'International plan',\n",
       "       'Voice mail plan', 'Number vmail messages', 'Total day minutes',\n",
       "       'Total day calls', 'Total day charge', 'Total eve minutes',\n",
       "       'Total eve calls', 'Total eve charge', 'Total night minutes',\n",
       "       'Total night calls', 'Total night charge', 'Total intl minutes',\n",
       "       'Total intl calls', 'Total intl charge', 'Customer service calls',\n",
       "       'Churn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = ['Account length', 'Number vmail messages', 'Total day minutes', 'Total day calls', 'Total day charge', 'Total eve minutes', 'Total eve calls', 'Total eve charge', 'Total night minutes', 'Total night calls', 'Total night charge', 'Total intl minutes', 'Total intl calls', 'Total intl charge', 'Customer service calls']\n",
    "categorical_columns = ['State', 'Area code', 'International plan', 'Voice mail plan', 'Churn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical feature preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: \n",
    "\n",
    "For the Area code feature, we can perform OHE given it only has 3 unique values and they appear in both train and test sets.\n",
    "\n",
    "Since the State feature contains 51 unique values, applying One-Hot Encoding (OHE) would result in a large number of new features. Given that our dataset has only 2,666 entries, this could lead to the curse of dimensionality and potential overfitting. To mitigate this, two alternative encoding methods are considered:\n",
    "\n",
    "- Label Encoding: Assigns a unique numeric value to each state. However, this method may introduce an unintended ordinal relationship among categorical values.\n",
    "- Frequency Encoding: Replaces each state with its relative occurrence in the dataset, capturing its distribution without implying any order.\n",
    "\n",
    "To avoid data ;eakage in test dataset:\n",
    "- Label Encoding: The mapping derived from the training set should be applied to the test set, and any unseen categories should be assigned a default value 0.\n",
    "- Frequency Encoding: The frequency mapping from the training set should be used for the test set, and any unseen states should be assigned the mean frequency from the training distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([415, 408, 510], dtype=int64), 51)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Area code'].unique(), train_df['State'].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([408, 415, 510], dtype=int64), 51)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Area code'].unique(), test_df['State'].unique().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_binary_encoding(train_df, test_df, columns):\n",
    "    for column in columns:\n",
    "        train_df[column] = train_df[column].map({'Yes': 1, 'No': 0})\n",
    "        test_df[column] = test_df[column].map({'Yes': 1, 'No': 0})\n",
    "    return train_df, test_df\n",
    "\n",
    "def perform_one_hot_encoding(train_df, test_df, columns):\n",
    "    train_df = pd.get_dummies(train_df, columns=columns)\n",
    "    test_df = pd.get_dummies(test_df, columns=columns)\n",
    "    return train_df, test_df\n",
    "\n",
    "def perform_frequency_encoding(train_df, test_df, columns):\n",
    "    for column in columns:\n",
    "        new_col_name = column + '_freq'\n",
    "        freq_encoding = train_df[column].value_counts(normalize=True)\n",
    "        train_df[new_col_name] = train_df[column].map(freq_encoding)\n",
    "        test_df[new_col_name] = test_df[column].map(freq_encoding)\n",
    "        test_df[new_col_name].fillna(freq_encoding.mean(), inplace=True)\n",
    "    return train_df, test_df\n",
    "\n",
    "def perform_label_encoding(train_df, test_df, columns):\n",
    "    for column in columns:\n",
    "        new_col_name = column + '_label'\n",
    "        label_encoding = {k: i for i, k in enumerate(train_df[column].unique(), 0)}\n",
    "        train_df[new_col_name] = train_df[column].map(label_encoding)\n",
    "        test_df[new_col_name] = test_df[column].map(label_encoding)\n",
    "    return train_df, test_df\n",
    "\n",
    "def drop_columns(train_df, test_df, columns):\n",
    "    return train_df.drop(columns=columns, axis=1), test_df.drop(columns=columns, axis=1)\n",
    "\n",
    "def rearrage_columns(train_df, test_df, columns, target):\n",
    "    for column in columns:\n",
    "        freq_col = column + '_freq'\n",
    "        label_col = column + '_label'\n",
    "        ohe_col = 'ohe_' + column\n",
    "        cols = train_df.columns.tolist()\n",
    "        raw_idx = cols.index(column)\n",
    "        cols.insert(raw_idx + 1, cols.pop(cols.index(freq_col)))\n",
    "        cols.insert(raw_idx + 2, cols.pop(cols.index(label_col)))\n",
    "        train_df = train_df[cols]\n",
    "        test_df = test_df[cols]\n",
    "    \n",
    "    # Move target column to the end\n",
    "    cols = train_df.columns.tolist()\n",
    "    cols.insert(len(cols), cols.pop(cols.index(target)))\n",
    "    train_df = train_df[cols]\n",
    "    test_df = test_df[cols]\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = perform_binary_encoding(train_df, test_df, ['International plan', 'Voice mail plan'])\n",
    "train_df, test_df = perform_one_hot_encoding(train_df, test_df, ['Area code'])\n",
    "train_df, test_df = perform_frequency_encoding(train_df, test_df, ['State'])\n",
    "train_df, test_df = perform_label_encoding(train_df, test_df, ['State'])\n",
    "train_df, test_df = rearrage_columns(train_df, test_df, ['State'], 'Churn')\n",
    "train_df, test_df = drop_columns(train_df, test_df, ['State'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical feature preprocessing\n",
    "\n",
    "To avoid data leakage, we fit the scaler only on the training set and use the fit scaler to transform both train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to normalize numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_scaling(train_df, test_df, columns, scaler):\n",
    "    train_df[columns] = scaler.fit_transform(train_df[columns])\n",
    "    test_df[columns] = scaler.transform(test_df[columns])\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "train_df_standard, test_df_standard = perform_scaling(train_df, test_df, numerical_columns, min_max_scaler)\n",
    "train_df_min_max, test_df_min_max = perform_scaling(train_df, test_df, numerical_columns, standard_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the preprocessed df as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    \"standard_scale\": \n",
    "    {\n",
    "        \"train\": train_df_standard,\n",
    "        \"test\": test_df_standard\n",
    "    },\n",
    "\n",
    "    \"min_max_scale\": \n",
    "    {\n",
    "        \"train\": train_df_min_max,\n",
    "        \"test\": test_df_min_max\n",
    "    } \n",
    "}\n",
    "\n",
    "with open('./dataset/cleaned_datasets.pkl', 'wb') as f:\n",
    "    pickle.dump(data_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS5228",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
